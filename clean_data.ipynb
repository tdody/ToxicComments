{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python389jvsc74a57bd0285f0c52ca504ea4f828ad46c10e09bbb808792a6a820ce246b5e6249fdc20a9",
   "display_name": "Python 3.8.9 64-bit ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "285f0c52ca504ea4f828ad46c10e09bbb808792a6a820ce246b5e6249fdc20a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "path = 'Dataset/'\n",
    "TRAIN_DATA_FILE=path + 'train.csv'\n",
    "TEST_DATA_FILE=path + 'test.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "68it [00:00, 37592.29it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "########################################\n",
    "# Load the cleaned words\n",
    "########################################\n",
    "\n",
    "cl_path = 'features/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in tqdm(cl):\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 266/159571 [00:00<01:00, 2654.95it/s]Processing text dataset\n",
      "100%|██████████| 159571/159571 [00:47<00:00, 3392.38it/s]\n",
      "100%|██████████| 153164/153164 [00:45<00:00, 3392.91it/s]Cleaned.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^?!.,:a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "word_count_dict = defaultdict(int)\n",
    "toxic_dict = {}\n",
    "\n",
    "def clean_text(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        # Drop the image\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.jpg\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.png\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.gif\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*\\.bmp\", \" \", text)\n",
    "\n",
    "        # Drop css\n",
    "        text = re.sub(r\"#([A-Fa-f0-9]{6}|[A-Fa-f0-9]{3})\", \" \",text)\n",
    "        text = re.sub(r\"\\{\\|[^\\}]*\\|\\}\", \" \", text)\n",
    "        \n",
    "        # Clean templates\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[user:.*\\|\", \" \", text)        \n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[wikipedia:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[special:.*\\|\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\]\", \" \", text)\n",
    "        text = re.sub(r\"\\[?\\[category:.*\\|\", \" \", text)\n",
    "    \n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\?\", \" ? \", text)\n",
    "    text = re.sub(r\"\\!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\\"\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub(' ', text)\n",
    "    #text = special_character_removal.sub('',text)\n",
    "\n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return (text)\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "\n",
    "comments = [clean_text(text) for text in tqdm(list_sentences_train)]    \n",
    "test_comments=[clean_text(text) for text in tqdm(list_sentences_test)]\n",
    "\n",
    "print(\"Cleaned.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}